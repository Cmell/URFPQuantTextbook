[["index.html", "URFP Quantitative Project Textbook Chapter 1 Introduction 1.1 R Help 1.2 Using R Studio 1.3 Packages This Book Uses 1.4 Sources", " URFP Quantitative Project Textbook Christopher Mellinger, Ph.D. 2023-04-17 Chapter 1 Introduction This book is intended to cover just enough statistical content to help students in the Undergraduate Research Fellowship Program (URFP) at the Renée Crown Wellness Institute perform analyses of UPWARD data. This book covers: Basic data operations. Regression. Within-participant analysis of two repeated measures via difference scores and regression. Moderation with regression, allowing both continuous and categorical variables. Mediation. Each of these topics has been well-documented in other sources. The approach here will be to provide basic examples of useful skills, but then refer the reader to other links that elaborate and provide more foundation. 1.1 R Help It is helpful to know about R’s built-in documentation system. At any point, you can use the console to access manuals for any R function or dataset. Just type: ?mean The help page for that function appears in the help pane in R Studio. 1.2 Using R Studio I highly recommend that you learn to take advantage of R Studio! Read this introduction section. You might also look at the RStudio cheat sheet. 1.3 Packages This Book Uses This book relies on the packages loaded in the next chunk: ## ── Attaching core tidyverse packages ──────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.1 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.2 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors This book additionally uses these packages, but you probably don’t need them: # packages we don&#39;t need people install library(flextable) library(ggbeeswarm) If you need to, you can install the tidyverse this way: install.packages(&#39;tidyverse&#39;) The CMUtils package was developed by the author of this textbook and lives on github. Install it like this: install.packages(&#39;devtools&#39;) devtools::install_github(&#39;Cmell/CMUtils&#39;) 1.4 Sources This book was created with the packages above as well as bookdown Wickham (2023). The methods presented here are largely based on the model comparison approach to statistical analysis (Judd, McClelland, and Ryan 2017). "],["basicops.html", "Chapter 2 Data Operaions", " Chapter 2 Data Operaions There is a wealth of information on basic data manipulation available. For the purposes of this project, I highly recommend reading these sections of the Tidyverse cookbook: Tidy: sections 6 through 9 will be particularly useful. Transform: sections 1-6, 8-10, 13, 14, adn 23 cover working with tables (tibbles or dataframes); sections 15-18 will be useful for working with varibles inside tables. Visualize: nice compact intro to ggplot, which is the framework I’ll use throughout this book. "],["regression.html", "Chapter 3 Regression 3.1 What Is Regression? 3.2 Intepreting Regression Output 3.3 Centering 3.4 Interpreting Regression with Mean-Centered Predictors 3.5 Chapter Takeaways", " Chapter 3 Regression You likely have seen some regerssion models. This following tutorial focuses on interpretation of each element in a regession that will be important later for moderation or mediation analyses. 3.1 What Is Regression? As a simple example, consider the mtcars dataset. We might be interested in relating two variables, say, the weight of the car in tons and the fuel efficiency in miles per gallon (mpg). The most familiar way to perform this analysis is with Pearsion’s correlation, but it turns out that using regression is totally equivalent. Here’s the code for running that regression in R and seeing the result. # I&#39;m making a variable to hold the output of the regression; you should always # name your variables something informative; if you copy this code, change that # variable name! efficiencyModel = lm( mpg ~ wt, # specify the model. data = mtcars # specify the dataframe that we want to use ) # now view the result lmSummary(efficiencyModel) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.8576 394.3233 0.929 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.5590 91.3753 0.753 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 mtcars is a toy dataset that is loaded in R’s environment by default. You can look its information by using R help: ?mtcars Before I explain what’s going on, I’ll provide a write up of these results. Skim it over and try to get the main gist. By the end of this chapter, we hope to make sense of each piece. Car efficiency was regressed on car weight to examine the relationship between mass and fuel use. A significant, inverse relationship between efficiency and weight emerged, \\(b = -5.345, F(1,30) = 91.38, p &lt; .001\\). Note that the term “significant” here is used differently than its collquial meaning. What that really means in the write up above is “statistically reliable.” We’ll explore that more later. Returning to the output,what does it all mean? The model that R estimated for us is actually this: \\[\\widehat{mpg_i} = 37.29 - 5.34 * wt_i\\] We can see our two variables, mpg and wt, in that equation. Notice that each of them has a subscript, i. This shows the fact that we are using data from every single person. We can choose i to represent any given car in the mtcars dataset and apply our equation. Another notable feature is the hat over the mpg variable. That hat represents the fact that we are predicting, or making a guess, for each car’s MPG value via this equation. Finally, there are two number, 37.29 and -5.34. These come from the regression output above. Notice the numbers under the heading, “Estimate” in the table in the middle of the ouptut. One is labelled “Intercept,” and the other “wt.” Let’s see what these guesses turn out to be. I’m going to apply this equation to the first several rows in the mtcars dataset. Here are the values we are starting with: .cl-bc27951a{}.cl-bc1dbdb0{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bc2234d0{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bc22532a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bc225334{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bc225335{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}mpgwt21.02.62021.02.87522.82.32021.43.21518.73.44018.13.460 Let’s use our equation on the very first row. We simply substitute the values in that row for the placeholders in the equation above: \\[\\widehat{mpg_1} = 37.29 - 5.34 * 2.62 = 23.2992\\] Remember, the \\(\\widehat{mpg_1}\\) is a predicted mpg, so we do not substitute there. Notice that I replaced the i with a 1 because we are applying our model to the first car in the dataset. We get a prediction of 23.2992 MPG. The actual MPG is listed in the table above as 21.0, so we are off by a little bit. But that’s ok because we are using our regression to summarize this information, not get every number exactly right. I can repeat this process for every car in the dataset. Here’s what the first few cars predictions look like: mtcars %&gt;% select(mpg, wt) %&gt;% head() %&gt;% mutate( # calculate our predictions `mpg_hat` = 37.29 - 5.34 * wt ) %&gt;% flextable() %&gt;% colformat_double(j = &#39;mpg_hat&#39;, digits = 3) .cl-bcc47790{}.cl-bcbb585e{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bcbef37e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bcbf0922{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bcbf092c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bcbf0936{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}mpgwtmpg_hat21.02.62023.29921.02.87521.93822.82.32024.90121.43.21520.12218.73.44018.92018.13.46018.814 We know that we can use our equation to calculated estimated mpg values for each car. But why would we do that? Well, we don’t often need the exact predictions. But it is useful to know that the model is trying to do this. Let’s represent the model graphically. wt_mpg_plot = mtcars %&gt;% ggplot( aes(x = wt, y = mpg) ) + geom_point() + # add points for each actual car # add a line with values that are suspiciously familiar: geom_abline( intercept = 37.29, slope = -5.34, color = &#39;blue&#39; ) wt_mpg_plot We have exact analogs of everything in the tables above. Each point represents a car’s weight on the x axis, and its efficiency in MPG on the y axis. Finally, the line represents those predicts from before. If you take a particular car’s weight and find the spot directly above it on the blue line, the place on the y-axis directly to the left of the line will be the prediction for that car. I’ll mark that on the next graph for the first car in our dataset. # just adding elements to the same plot we had before wt_mpg_plot + geom_segment( x = 2.62, # the weight of the first car xend = 2.62, # repeat it to make the function happy y = 0, yend = 23.2992, color = &#39;forestgreen&#39; ) + # add the horizontal line; geom_segment( x = 0, xend = 2.62, y = 23.2992, yend = 23.2992, # the prediction for mpg color = &#39;forestgreen&#39; ) Notice that the vertical green line passes right through the point for the first car in our dataset. If we follow the horizontal green line from its intersection with the blue line, we end up at our prediction for the first car. The takeaway here is that we can understand our model as describing predictions for each car in the dataset using just two numbers. I’m going to formally give these names and interpretations now: The intercept tells us the predicted efficiency of a car that has a weight of zero. The slope tells us how much we expect efficiency to change for a 1-ton weight difference. These interpretations are critical. We’ll grapple with them more soon, but for now, I’ll repeat them in more general language. These two things should become mantras for you as you interpret regressions: The intercept is the predicted value of an outcome for those cases with a value of zero on all predictors. The slope is the predicted change in the value of the outcome for a 1 unit change in the value of its predictor. And finally, one more time, very concretely with the mtcars dataset: Our intercept of 37.29 is the predicted efficiency, in miles per gallon, of a car that weighs 0 tons. Obviously a car can’t weigh zero tons, so this value is not particularly meaningful for us in this model. We will change that soon. The slope of -5.34 tells us that if car B weighs 1 ton more than car A, car B is predicted to have an efficiency of 5.34 mpg less than car A. There is a negative relationship between car weight and car efficiency in this sample. Now that we have interpretations of these pieces of the model, let’s look at some of the other pieces of the output. 3.2 Intepreting Regression Output Here’s the output again. lmSummary(efficiencyModel) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.8576 394.3233 0.929 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.5590 91.3753 0.753 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 We’ve looked at the “Estimate” column and now know that those values describe a set of predictions using an intercept and a slope. Let’s take a look at two other columns that will be useful for us. The first is labelled “f value.” Describing the precise meaning of F is outside the scope of this article, but know that it indexes the statistical precision that our model brings. Remember how our model’s predictions were never precisely the real values? F is one way of understanding something about how accurate1 those predictions were, taking into account model complexity. Larger F values mean our model was more accurate. Values of around 3 or 4 are usually statistically significant, though that depends somewhat on sample size. Our F of more than 90 in this case tells us that we have a model that is quite good, at least in an inferential sense. We derive confidence2 in the fact that we have a negative relationship between efficiency and weight. The second column we examine is labelled “Pr(&gt;|t|)” which is a mathematical way of saying “p value.” I am not going to reiterate the null hypothesis statistical test framework here. Instead, I’ll refer you to the another explanation of this if you want the in depth look. In the meantime, let me give you interpretations of p that are correct, and those that are not. Here are good ways to describe p: Our low p-value of less than .001 indicates that under a null hypothesis of no association between efficiency and car weight, it is unlikely that we would see a set of cars that look this way from sampling error alone. Consider a hypothetical world in which the correlation between MPG and car weight was 0. That is, if we could measure all existing cars’ weights and efficiencies, we would see no indication of a relationship between those two variables. In this hypothetical world, it’s unlikely that we would get a set of cars showing the negative association if we randomly sampled just 32 cars. Because there’s a low chance of that happening (p&lt;.001), we might assume that we are not in the hypothetical world with no association; the only possibility left is that we are in a world where there is a relationship bewteen efficiency and weight. Here’s some ways that people commonly describe p values, but they are not totally accurate. The incorrect statement is italicized, and then my explanation is in normal font. Our low p value indicates that there’s a low chance that our F value is due to random chance. This is accurate only if you mean “sampling error under the null hypothesis” when you say “random chance.” But “random chance” could mean a lot of things, so I don’t love it. The p value is the probability that our data are random. In a strict sense, all data in a sample are random as long as the sampling procedure had some element of randomness to it. And for an accurate study, randomness is essential. If we don’t randomly sample, then we cannot have confidence that our result generalizes. But now we’re deep in the weeds of generalizability theory, and we don’t need to stay here. Basically, this way of describing a p value is not right. Ok, so what should you do with p values? Revist my write up above. By convention, we simply note whether p is less than .05. If so, we consider ourselve to have “enough evidence” that we conclude an effect. If not, we are still in the dark; our study was not precise enough to speak to the reality. 3.3 Centering We know now that the intercept is the predicted value when the other predictor is zero. Wouldn’t it be nice if we had a way to make “0” a meaningful value? Enter variable centering. Consider our plot from before, without the line. mtcars %&gt;% ggplot( aes(x = wt, y = mpg) ) + geom_point() Zero does not even appear on the x axis because there are no cars with a weight of zero. But watch what happens when I subtract a constant from wt: mtcars %&gt;% mutate( new_wt = wt - 2 # subtract a constant, 2, from wt ) %&gt;% ggplot( aes(x = new_wt, y = mpg) ) + geom_point() Visually, it looks identical if you ignore the x axis. But comparing the x axis, there is a big difference. Let’s put them side by side so we can see it: bind_rows( mtcars %&gt;% mutate(panel=&quot;wt&quot;), mtcars %&gt;% mutate(wt = wt - 2, panel=&quot;wt - 2&quot;) ) %&gt;% ggplot( aes(x = wt, y = mpg) ) + geom_point()+ facet_wrap( vars(panel), nrow = 2, scale = &quot;free_x&quot; ) The x-axis has shifted, but the relative position of points is identical in the two plots. This is an important feature! Notice what happens when we draw a line through those two plots. I will use a slightly different method here, but the line will appear the same as above: bind_rows( mtcars %&gt;% mutate(panel=&quot;wt&quot;), mtcars %&gt;% mutate(wt = wt - 2, panel=&quot;wt - 2&quot;) ) %&gt;% ggplot( aes(x = wt, y = mpg) ) + geom_point() + geom_smooth(method=&quot;lm&quot;) + facet_wrap( vars(panel), nrow = 2, scale = &quot;free_x&quot; ) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Just like the data points, the line looks identical! The only difference is the placement of the x-axis. The axis has been shifted to the right in the wt-2 plot relative to the wt plot. Recall that the relationship between efficiency and weight is estimated in the slope. This is key; we centered our variable by subtracting a constant, but the slope of the best-fit line remains unchanged. That is, we can assess the relationship between our variables in exactly the same way as before, by examining the slope. The final feature of this shift that is critical for our purpose is the way we control the zero point of the axis. Notice that the value 2 in the original wt data aligns with the new wt-2 data value of 0. This is not a coincidence. We subtracted 2, and our new data now has a value of 0 at what use to be 2. In fact, we can intentionally alter where zero lands in our data by subtracting the number we want represented by 0. For example, a very interesting value that we will often subtract is the mean of the data we are centering. This strategy is known as mean centering a variable. We will rely on this extensively in the next chapter, so it is worth understanding! To mean-center our weight variable, we can simply do this: # Create a new dataframe called mtcars_2 that we can use # later. mtcars_2 = mtcars %&gt;% mutate( wt_C = wt - mean(wt) # mean center weight ) The expression mean(wt) results in the mean of all weights in the dataset, that is, it results in a constant value. That constant in our case is 3.217. Let’s graph our values as before. mtcars_2 %&gt;% ggplot( aes(x = wt_C, y = mpg) # use the centered weight variable. ) + geom_point() As we have come to expect, the relative position of points remains identical, but 0 falls in a new spot on our x axis. Refer to the earlier plots and you can see that 0 is where the mean, 3.217, used to be. 3.4 Interpreting Regression with Mean-Centered Predictors Let’s run a regression and use the tools of interpretation we developed earlier. efficiencyModelWtC &lt;- lm( mpg ~ wt_C, data = mtcars_2 ) lmSummary(efficiencyModelWtC) ## ## Call: ## lm(formula = mpg ~ wt_C, data = mtcars_2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 20.0906 0.5384 37.3126 1392.2290 0.979 &lt; 2e-16 *** ## wt_C -5.3445 0.5591 -9.5590 91.3753 0.753 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 Our syntax did not change at all except to use our new wt_C variable, and our new mtcars_2 dataframe. As you might have suspected, the slope is identical to our original model at -5.3445. The F value is also identical at 91.38. The parameters that characterize the relationship between our two variables have remained exactly as they were before, further increasing our confidence that centering our weight variable still allows us to see that a 1 ton difference between two cars results in a prediction of 5.345 fewer MPGs in the heavier car than in the lighter one. However, the intercept has changed; it is now 20.091. If we apply our rules of interpretation, we have this: Cars with 0 on the wt_C variable are predicted to have an efficiency of 20.091 MPG. What kind of car has a value of 0 on wt_C? We know that shifting our x axis allowed use to place 0 on our new variable at the mean of wt in our original variable. That is, the kind of car that has 0 on wt_C is a car with exactly average weight. We may or may not have an actual car in our dataset that weighs exactly the mean of all other cars, but we are in the business of prediction, so that is ok. Let’s rephrase our interpretation to reflect this fact: A car of average weight is predicted to have an efficiency of 20.091 MPG. Recall that the statistical test of the intercept tests whether or not that value, regardless of its interpretability, is different from zero or not. The F value is 1392.23, with a p value of less than .001. As a result, we conclude that our MPG prediction for a car of average weight is reliably different from zero in the population of cars that mtcars sampled from. One final connection. Our intercept is now estimated as exactly the mean MPG in our dataset: mean(mtcars$mpg) ## [1] 20.09062 If all predictors in a regression are mean-centered, then the intercept necessarily reflects the mean of the outcome variable. In the case of the cars dataset, the statistical conclusion yeilded from this test is a bit obvious: the cars weigh more than zero, on average. However, there are many psychological outcomes where we care very much about whether the average of the outcome deviates from zero. We will see an example where this is one of our critical tests in the next chapter. In general, mean-centering predictors usually has no downside.3 If all predictors in a regression4 are mean-centered, then the intercept becomes much more interpretable, usually. 3.5 Chapter Takeaways Regression with a single predictor characterizes a dataset by estimating a slope and intercept. These describe a line which can be understood as the model’s predictions for each datapoint and other, hypothetical, datapoints that we did not observe. The slope is always interpreted as the predicted increase in the outcome variable for a 1-unit increase in the predictor variable. The intercept is always interpreted as the average value of the outcome variable for a datapoint with a value of zero on the predictor variable(s). For our purposes here, we compare the p value to .05. If it is less than .05, we understand that our data (or more extreme data) were unlikely to occur in a hypothetical world where no effect (no relationship for a slope, and the average is not different from zero for an intercept) exists. We derive our p value from the F value, which indexes the “accuracy” of our model’s predictions. Usually an F of 3 to 4 is enough to be significant, but this depends on sample size. We can choose where zero on our predictor variable lands by centering. Mean centering is usually a useful choice. This might help use interpret our intercept as the mean value of our outcome variable. It can be useful to test this mean value against zero. Sort of. Really, it is a measure of how much variation we accounted for in our model as compared to a simpler model, e.g., one which sets the slope to zero. So really, it is an index of how much better our model is than one where we don’t try to use the statistic of interest. This gets to be a deep, and sometimes controversial, topic. The description here is intended to give you an intuitive sense of what’s going on. For data analysis in a professional capacity, it is important to get familiar with null hypothesis statistical testing, its philosophy, and alternatives like Bayesian analysis.↩︎ Again, this is an intuitive explanation that sacrifices a lot of nuance and meaning in service of getting off the ground quickly. Confidence is a slippery term, and it is probably not true that we can quantify our confidence in the framework of null hypothesis tests, like the F statistic. But many people still use and expect this statistic to index confidence, and in a practical sense it probably does ok.↩︎ At least, there is no downside in the kind of models we are discussing here. For more advanced models, like logistic regression or multilevel modeling, great care must be taken in centering.↩︎ We can certainly have more than one predictor in a regression. You may have encountered this as multiple regression. We will rely on mutliple regression techniques later for both moderation and mediation.↩︎ "],["within-participant-analysis.html", "Chapter 4 Within-Participant Analysis 4.1 Example: Chick Weights 4.2 Computing Difference Scores 4.3 Using Regression 4.4 Important Aside: Contrast &amp; Dummy Codes 4.5 Predicting a Difference Score 4.6 Other Possibilities 4.7 Chapter Takeaways", " Chapter 4 Within-Participant Analysis There are lots of situations where we might have multiple measures of the same units of analysis. We could imagine measure a car’s efficiency, and then making an upgrade to the engine, and measuring its efficiency after the upgrade. Perhaps we could test a tutoring intervention by giving students a test on a topic, allowing them to get tutored, and then measuring their knowledge post-tutoring session. To be precise, we typically think of independent variables as either varying within participants or between participants. Variables that vary within participants imply more than one measurement of a variable. In the tutoring example, students had to take the test twice, once before and once after the tutoring session. We could describe our analysis this way: We examined student test scores by timepoint (before tutoring vs. after tutoring), which varied within partipants. In contrast, variables that vary between participants have only one value for each person/unit. Consider modifying the tutoring study. Let’s make it an experiment in which participants are randomly assigned to receive tutoring or not. Then we give the test to all students. Some students have the value “tutored” on the condition variable, while others have the value “control.” We would describe our analysis of this between-student variable this way: We examined student test scores by condition (tutored vs. control), which varied between participants. There are also mixed designs, where some factors (i.e., variables) vary between participants and some vary within. UPWARD is a mixed design. The primary indepedent variable in UPWARD was condition. Some participants were assigned to receive Mindfulness-Based Cognitive Therapy (MBCT) while other were assigned to Usual Care (UC). However, participants were also measure on several outcomes at many different timepoints. Participants completed the PHQ-9 many times so that the researchers knew their depressive symptom burden across the course of the study. If we analyze both time and condition, we could describe the analysis this way: We analyzed depressive symptom burden via PHQ-9 scores in a 2 (time: pre-intervention vs. post-intervention) x 2 (condition: MBCT vs. UC) analysis, with the first factor varying within participants and the second varying between. This kind of description is very common. The numbers refer to how many levels there are in a factor (i.e., variable) in the design. The “x” can be read as “by,” so the statement could be rephrased this way: “Our analysis was a 2 by 2 design.” We are going to unpack all of this as we go through this chapter. 4.1 Example: Chick Weights For the example in this chapter, we will focus on the ChickWeight dataset. To keep things simple, we are going to focus on only two timepoints in the dataset. The chunk below selects the rows we want and puts the data in wide form. For each chick, we are interested in its weight at birth and on day 2 of it’s life. chick = ChickWeight %&gt;% filter(Time %in% 0:2) %&gt;% pivot_wider( names_from = &#39;Time&#39;, names_prefix = &#39;weight_&#39;, values_from = &#39;weight&#39;, id_cols = c(&#39;Chick&#39;, &#39;Diet&#39;) ) %&gt;% mutate( Diet = ifelse( Diet %in% 1:2, 1, 2 ) ) In this study, researchers measured the weight of baby chickens (chicks) many times. They also varied the diet that those chicks consumed. The study included facotrs that varied both within chicks (time) and between chicks (diet; there were 4 diets in the original data but we will collapse that to 2 diets to keep things simple). Here’s what the first few rows of the data look like: chick %&gt;% head() %&gt;% flextable() .cl-bf589072{}.cl-bf4e50da{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bf522930{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bf522944{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bf523e52{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf523e5c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf523e5d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf523e5e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf523e66{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf523e70{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ChickDietweight_0weight_2114251214049314339414249514142614149 4.2 Computing Difference Scores Our primary interest is whether chicks weigh more, on average, on day 2 than on day 0. We have all the tools we need to answer this question from Chapter 3, with one exception. Notice that in the regression chapter, we had a single variable, MPG, as the outcome. Here, we have two variables we are dealing with: a weight on day 0, and a weight on day 2 (weight_0 and weight_2 respectively). What we really want is a variable that shows the extent to which a chick weighs more on day 2 than on day 0. It is simple to calculate this variable: we simply subtract weight_0 from weight_2 for each chick. Take a look at the result: chick_2 = chick %&gt;% mutate( weight_diff = weight_2 - weight_0 ) chick_2 %&gt;% head() %&gt;% flextable() .cl-bf714b80{}.cl-bf67ece8{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bf6bb33c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bf6bb350{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bf6bca02{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf6bca0c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf6bca0d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf6bca16{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf6bca17{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf6bca20{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ChickDietweight_0weight_2weight_diff11425192140499314339-4414249751414216141498 We can see that the first chick weighed 9 grams more on day 2 than on day 1. The second chick also weighed 9 grams more on day 2 than 0. The third chick weighed 4 grams less on day 2 than on day 0. We know this because the difference is negative; that only occurs if we subtract a larger number from a smaller one. It might seem obvious, but it is worth remembering these rules for interpreting difference scores: Positive numbers indicate growth, i.e., larger scores on the second variable than on the first. Zero indicates no change at all. The two numbers are equal. Negative numbers indicate shrinking, i.e., the first number is larger than the second. Note that these rules depend on subtracting in a particular direction. Carefully choose your subraction direction based on the situation of your analysis. 4.3 Using Regression Let’s analyze our chick weight difference scores. We use a slightly different syntax in the following model call. jNotice that the model I specify is weight_diff ~ 1. The “1” tells R that we do not have any variables we are using as predictors, but that it should still estimate an intercept for our model. wt_diff_model = lm( weight_diff ~ 1, data = chick_2 ) lmSummary(wt_diff_model) ## ## Call: ## lm(formula = weight_diff ~ 1, data = chick_2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.16 -1.16 -0.16 1.59 5.84 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 8.160 0.513 15.907 253.032 0.838 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.627 on 49 degrees of freedom Let’s apply our mantras for interpretting regression. We only have an intercept, so here is the interpretation: On average, chicks weighed 8.16 grams more on day 2 than on day 0, \\(b = 8.16, F(1, 49) = 253.03, p &lt; .001\\) That’s it! In the case of only 2 measurements, difference scores are straightforward to analyze. We simply take the mean of a difference score (via an intercept in regression), statistically test it against zero, and interpret accordingly. 4.4 Important Aside: Contrast &amp; Dummy Codes Recall that our chicks were fed one of two diets, labelled diet 1 and diet 2. We might want to know if chicks on one of those diets grew more than chicks on another diet. Notice that with our regression strategy, we need all our variables to be represented as numbers. In this particular dataset, the diets are already represented as numbers, but there’s at least two reason we want to recode. First, we want to choose clever values for our codes so that our regression output is asking specific questions of our data. Second, not all datasets come pre-coded with values; many will have character strings or boolean values (TRUE/FALSE) that encode condition, and we’ll need to replace them anyway. Let’s see how many people we have in each condition: chick_2 %&gt;% count(Diet) %&gt;% flextable() .cl-bf8ae4d2{}.cl-bf7f97e4{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bf84dc22{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bf84f496{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf84f4a0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf84f4aa{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}Dietn130220 We will use two different coding schemes: contrast codes and dummy codes. The primary difference in these coding schemes is where zero ends up. That’s critical for any analysis in which we want to interpret the intercept.5 4.4.1 Contrast coding scheme for diet \\[ dietContrast = \\begin{cases} -.5 &amp; \\quad \\text{if diet 1}\\\\ +.5 &amp; \\quad \\text{if diet 2} \\end{cases} \\] To make this happen, here is a common coding motif that works well for recoding. chick_3 &lt;- chick_2 %&gt;% mutate( dietContrast = -.5 * (Diet==1) + .5 * (Diet==2) ) This code works because both of the expressions in parenthese, (Diet==1) and (Diet==2), actually evaluate to logical vectors (try this code in your console if you are curious: ChickWeight$Diet==1). That’s why the double-equals sign is there. A TRUE value to a computer is equivalent to 1, where a FALSE value is equivalent to zero. By multiplying our codes against this vector of TRUE/FALSE or 0/1, we end up with the code when the expression is true and a 0 when the expression is false. Adding them together, each row will get the right value. Zero, in this scheme, falls approximately at the average of our chicks. This depends somewhat on how balanced our dataset is. In this case, we have 20 chicks in one condition and 30 in the other, so it won’t be precisely the average. But it is close enough to ask the statistical question, “is the intercept different from zero for chicks on average?” 4.4.2 Dummy coding scheme for diet For dummy coding schemes, we always assign one of the groups to zero and the other to one. \\[ dietDummy1 = \\begin{cases} 0 &amp; \\quad \\text{if diet 1}\\\\ +1 &amp; \\quad \\text{if diet 2} \\end{cases} \\] I like to name my dummy coded variables after the group that is coded zero because I know I will interpret the intercept as estimated for that group. Other people have different preferences, and you might see some people name their variables after the group coded 1 (which has advantages in certain situations). chick_4 &lt;- chick_3 %&gt;% mutate( dietDummy1 = 0 * (Diet==1) + 1 * (Diet==2), # we might also want to go the other direction dietDummy2 = 1 * (Diet==1) + 0 * (Diet==2) ) 4.5 Predicting a Difference Score We do not have to be satisfied with a simple difference score analysis. We can also predict our difference score from another variable! For this example, we are going to predict our difference score with the codes we generated in Section 4.4. First, I’ll regress weight_diff on dietContrast. wt_diff_dietc_model = lm( weight_diff ~ dietContrast, data = chick_4 ) lmSummary(wt_diff_dietc_model) ## ## Call: ## lm(formula = weight_diff ~ dietContrast, data = chick_4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8 -1.2 0.8 2.2 7.2 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 8.5000 0.4686 18.1378 328.9802 0.873 &lt; 2e-16 *** ## dietContrast 3.4000 0.9373 3.6276 13.1592 0.215 0.000692 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.247 on 48 degrees of freedom ## Multiple R-squared: 0.2152, Adjusted R-squared: 0.1988 ## F-statistic: 13.16 on 1 and 48 DF, p-value: 0.0006917 Let’s graph this as well to aid our interpretation. chick_4 %&gt;% ggplot( aes(x = dietContrast, y = weight_diff) ) + geom_point( # move each point slightly right or left for visibility position = position_jitter(width=.01) ) + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Notice three things. First, all the points fall on either -.5 or +.5 on the x axis. This aligns with our coding scheme. In fact, I “jittered” the points right and left so they didn’t all just sit on top of each other, obscuring the full pattern. Second, our choice of codes puts exactly 1 unit on the x axis between our two conditions. Third, our y axis is now a difference score. You should think of it as “chick growth” because of the way we calculated it. Points that are higher on the y axis indicate more growth from day 0 to day 2. Critically, there is no indication of actual chick weight on this plot; it only graphs our difference score. We still fit a line, as before, because we are still predicting values of weight difference. Applying our interpretations: The intercept is the estimated value of chick weight growth for chicks at 0 on the contrast code. Since 0 is in the “middle” of the contrast values (-.5 and +.5), we interpret this as the average weight difference for chicks in our samples. The values are close to what they were before, \\(b = 8.5, F(1, 48) = 329.0, p &lt; .001\\). Chicks increased in weight significantly (statistically speaking) from day 0 to day 2. The slope is the predicted difference in outcome, weight_diff here, for a 1 unit change in the predictor, the contrast code. Here is where another clever feature of our coding scheme happens. Because there is exactly 1 unit of difference between our two diet conditions (see the x axis of the plot above), our slope indicates the average difference in chick growth going from diet 1 to diet 2. That is, chicks on diet 2 showed 3.4 more grams of growths than chicks on diet 1, on average. There is one more thing to do. I am going to run two more models, one for each dummy code we created earlier. wt_diff_diet1_model = lm( weight_diff ~ dietDummy1, data = chick_4 ) wt_diff_diet2_model = lm( weight_diff ~ dietDummy2, data = chick_4 ) lmSummary(wt_diff_diet1_model) ## ## Call: ## lm(formula = weight_diff ~ dietDummy1, data = chick_4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8 -1.2 0.8 2.2 7.2 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 6.8000 0.5928 11.4714 131.5921 0.733 2.35e-15 *** ## dietDummy1 3.4000 0.9373 3.6276 13.1592 0.215 0.000692 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.247 on 48 degrees of freedom ## Multiple R-squared: 0.2152, Adjusted R-squared: 0.1988 ## F-statistic: 13.16 on 1 and 48 DF, p-value: 0.0006917 lmSummary(wt_diff_diet2_model) ## ## Call: ## lm(formula = weight_diff ~ dietDummy2, data = chick_4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8 -1.2 0.8 2.2 7.2 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 10.2000 0.7260 14.0495 197.3881 0.804 &lt; 2e-16 *** ## dietDummy2 -3.4000 0.9373 -3.6276 13.1592 0.215 0.000692 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.247 on 48 degrees of freedom ## Multiple R-squared: 0.2152, Adjusted R-squared: 0.1988 ## F-statistic: 13.16 on 1 and 48 DF, p-value: 0.0006917 Look at the Estimate columns for each model. For these two models, the slope is exactly the same as it was in the contrast coded model, except that the one in the second model is negative. This is because our dummy codes, just like the contrast code, maintains a 1 unit difference between groups. As a challenge, can you explain why the slope in the model using dummyDiet2 is negative?6 What about the intercepts from these two models? Applying our interpretation mantra for intercepts, we would say that the intercept in the first model is the predicted value for weight growth when dummyDiet1 is zero, or that the intercept represents the average weight growth for those on diet 1. It is significantly different from zero, so chicks on diet 1 are still growing from day 0 to day 2, but less than those on diet 2. The intercept from the model using dummyDiet2 is larger than the intercept when using dummyDiet1. It is the average value of chicks on diet 2, following the same logic as before. Chicks on diet 2 also grow significantly from day 0 to day 2. Let me provide a full write up of these results. Chick weight was examined in a 2 (time: day 0 vs. day 2) x 2 (diet: 1 vs. 2) mixed model ANOVA with the first factor varying within chicks and the second factor varying between chicks. Averaging across diet, chicks showed significantly higher weights on day 2 than on day 1, \\(b = 8.50, F(1, 48) = 329.0, p &lt; .001\\), indicating overall growth in the first days of life. The growth trend was qualified by a significant interaction with diet, \\(b = 3.40, F(1, 48) = 13.16, p &lt; .001\\). Chicks on both diets showed significantly larger weights on day 2 than day 0, but growth was larger for diet 2,\\(b_{diet2}=10.20, F(1,48)=197.39, p&lt;.001\\), than for diet 1, \\(b_{diet1}=6.80, F(1,48)=131.59, p&lt;.001\\). 4.6 Other Possibilities You are not limited to contrast codes when regressing difference scores. You can regress difference scores on continuously measured variables as well. For example, we could imagine taking height measurements for each chick in our dataset. It would be a completely legimate use of regression modeling to regress our difference score on these height measurements. Note that we have restricted ourselves to two levels on our factors. There are ways to extend these analysis techniques to larger numbers of levels, 3, 4, 5, and so on. But this topic gets fairly involved. For the purpose of the URFP project, we’ll stick with these simpler models. 4.7 Chapter Takeaways Difference scores can be used to index the change between a variable measured twice (or more) for the same participants. We can regress difference scores just like any other variable. An intercept-only model, variable ~ 1, allows us to test whether the average difference is statistically distinguishable from zero. For factors that vary between participants, we use contrast codes and dummy codes to allow regression-based analysis. The primary difference between contrast and dummy codes is where 0 lands, which gives us precise control over the statistical questions we ask. We can predict difference scores in a regression from codes or continuous variables. And, indeed, other analyses where the zero points of variables matter. We’ll see this when we get to interactions between two factors that vary between participants.↩︎ Hint: look at the dummy codes we set up and remember that the slope asks a directional question.↩︎ "],["mediation.html", "Chapter 5 Mediation", " Chapter 5 Mediation I found an excellent resource online for mediation tests with regression. It is the pdf you can download at the link at the end of this article. Note, however, that the mediation analysis assumes one thing we have not touched on in this book yet: multiple regression. Multiple regression is much like “single” regression, except that more than one predictor is entered on the right-hand side of the regression equation. Here’s an example, going back to the cars dataset. mpg_wt_disp_model = lm( mpg ~ wt + disp, data = mtcars ) lmSummary(mpg_wt_disp_model) ## ## Call: ## lm(formula = mpg ~ wt + disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 34.96055 2.16454 16.15150 260.87087 0.900 4.91e-16 *** ## wt -3.35082 1.16413 -2.87840 8.28518 0.222 0.00743 ** ## disp -0.01773 0.00919 -1.92861 3.71953 0.114 0.06362 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 How do we interpret these values? It turns out, slopes maintain their interpretation, as do intercepts. Here are interpretations of each parameter: The intercept of 34.96 is the average MPG of a car that weighs 0 tons and has 0 displacement (displacement is a measure of engine size). If car A weighs 1 ton more than car B, then car A is predicted to have 3.35 fewer MPG than car B, controlling for displacement. If car A has 1 cubic inch more displacement than car B, then car A is predicted to have .018 fewer MPGs than car B, controlling for weight. These interpretations are remarkably similar to what we did in Chapter 3. It is important, now, to remember that the intercept is the predicted value of the outcome when all predictors are 0. One funny little phrase appears at the end of my interpretations for slopes: “controlling for displacement/weight.” When we enter multiple predictors in a regression, we interpret slopes as controlling for all other predictors in the regression. This means that we are looking at the weight slope, but statistically equating all cars on displacement. Likewise, we are looking at the displacement effect, but holding constant car weights (statistically). Thus, the slope for weight in this model is not directly comparable to the slope for weight in the models in Chapter 3. Those models asked: “is weight related to efficiency?” This model asks: “is weight related to efficiency if all cars had totally equal displacements?” Both questions are valid, and we will want to ask them in different situations. Now, go read the mediation lesson: Mediation analysis PDF "],["moderation.html", "Chapter 6 Moderation 6.1 Mathematical Explanation 6.2 Example Model 6.3 Interpretting Interactions 6.4 Extension to Continous Moderators", " Chapter 6 Moderation Moderation models ask the question: does the relationship between a predictor and the outcome differ based on the level of another predictor? Let’s return to the mtcars dataset for this analysis. mtcars %&gt;% head() %&gt;% flextable() .cl-bfee5616{}.cl-bfe318aa{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bfe73b92{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bfe753f2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bfe753fc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bfe75406{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}mpgcyldisphpdratwtqsecvsamgearcarb21.061601103.902.62016.46014421.061601103.902.87517.02014422.84108933.852.32018.61114121.462581103.083.21519.44103118.783601753.153.44017.02003218.162251052.763.46020.221031 We’ll focus on predicting efficienby (mpg) from weight again, but we’ll consider whether another feature of the vehicle changes the relatinoship we saw before: the transmission type. The variable am is coded 0 for cars with automatic transmissions and 1 for cars with manual transmissions. We learned about coding schemes in Section 4.4. This predictor is currently dummy-coded. Recall that we observed a negative relationship between effiency and weight in Chapter 3. We will additionally ask: is the inverse relationship the same for cars with automatic transmissions as it is for manual transmissions? Before we begin, let’s set up our data just how we want. I’m going to center our weight predictor around the mean so that we can interpret the intercept (and for another reason we’ll see shortly). I’m also going to set up a contrast code for the am variable, as well as the complementary dummy code. mtcars_3 = mtcars %&gt;% mutate( wtC = wt - mean(wt), amC = -.5 * (am==0) + .5 * (am==1), # -.5 if automatic, +.5 if manual amAuto = 1 *(am==1) + 0 * (am==1), # redundant, but I want to spell it out amMan = 0 * (am==1) + 1 * (am==0) # the dummy code the other direction ) 6.1 Mathematical Explanation To motivate our strategy, we’ll examine some easy algebra. Here is the model we estimated when there was just one predictor: \\[\\begin{equation} \\widehat{mpg_i} = b_0 + b * wtC_i \\tag{6.1} \\end{equation}\\] Take a minute to appreciate the interpretation of that equation. It says that as \\(wtC_i\\) gets larger, \\(mpg_i\\) is also predicted to get larger. The amount by which the two are tied together in their increase is estimated in \\(b\\). So, the bigger \\(b\\) is, the stronger the relationship between \\(wtC_i\\) and \\(\\widehat{mpg_i}\\). The critical thing is that we want the \\(mpg_i\\) ~ \\(wtC_i\\) relationship, which is represented by \\(b_1\\), to depend on transmission bype, represented by \\(amC_i\\). I’ll rephrase the point. If the relationship between efficiency and weight is encapsulated in the multiplier on weight, we want the multiplier on weight, \\(b\\), to be a function of transmision type. Here’s a way to write that down in an equaion. It might look a bit funky right now, but in a moment we’ll “unfunk” it: \\[\\begin{equation} \\widehat{mpg_i} = b_0 + (b_1 + b_3 * amC_i) * wtC_i + b_2 * amC_i \\tag{6.2} \\end{equation}\\] Notice that equation (6.2) has the property we asked for: the multiplier of \\(wtC_i\\) in the parentheses is a function of both \\(b_1\\) and also \\(amC_i\\). In a practical sense, this means that the larger that \\(amC_i\\) is, the larger the slope for \\(wtC_i\\). The slope is now “compound,” comprised of a free parameter \\(b1\\) and a parameter that depends on the magnitude of \\(amC_i\\). Now, let me expand the equation by distributing through the slope quantity and re-ordering the terms: \\[\\begin{equation} \\widehat{mpg_i} = b_0 + b_1 * wtC_i + b_2 * amC_i + b_3 * wtC_i * amC_i \\tag{6.3} \\end{equation}\\] This equation represents the way we will write our regressions. There are a few important features that must be present in each interaction model. First, note that both predictors of interest, weight and transmission type, have a term by themselves in the model: \\(b_1 * wtC_i\\) and \\(b_2 * amC_i\\). These are called the main effects of weight and transmission repectively. They ask useful questions. Are weight and transmission type related to efficiency, controlling for the other? Second, the interaction term is literally the multiplication of weight, transmission type, and the parameter representing the interaction, \\(b_3 * wtC_i * amC_i\\). The parameter on this term, \\(b_3\\), will provide a statistical test of whether the relationship between weight and efficiency depends on transmission type. Finally, notice that the equation above is completely symetrical with respect to which variable is considered the moderator and which is the “primary predictor.” That is, if we had only looked at equation (6.3) we would have no idea if the researcher was condering transmission type to be the moderator of the efficiency-weight relationship, or if weight was moderating the transmission-type-efficiency relationship. This is important: interactions are symetrical. When we build this model, we are simultaneously asking whether both whether transmission moderates weight’s influence, and whether weight moderates transmission’s influence on efficiency. 6.2 Example Model Let’s estimate our model according to equation (6.3). interactionModel = lm( mpg ~ wtC + amC + wtC * amC, data = mtcars_3 ) lmSummary(interactionModel) ## ## Call: ## lm(formula = mpg ~ wtC + amC + wtC * amC, data = mtcars_3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6004 -1.5446 -0.5325 0.9012 6.0909 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 18.1520 0.7094 25.5862 654.6556 0.959 &lt; 2e-16 *** ## wtC -6.4351 0.7223 -8.9085 79.3623 0.739 1.16e-09 *** ## amC -2.1677 1.4189 -1.5278 2.3341 0.077 0.13779 ## wtC:amC -5.2984 1.4447 -3.6674 13.4502 0.324 0.00102 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.591 on 28 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.8151 ## F-statistic: 46.57 on 3 and 28 DF, p-value: 5.209e-11 Let’s interpret the parameters we’ve seen before: Intercept: the average (we centered everything, so we’re ok to talk about this as the average) value of car efficiency is 18.15, and this is significantly different from 0, \\(F(1,28)=654.7, p&lt;.001\\). Of course, no car woulde work if it had an efficiency of 0 MPG, so we are not surprised by this. Weight: as weight increases by 1 ton, our model predicts that efficiency decreases by 6.43 MPGs, holding constant transmission type. That relationship is significantly different from 0, \\(F(1,28)=79.4, p&lt;.001\\). This is close to what we saw in Chapter 3. Transmission: moving from automatic to manual transmissions (am was contrast coded), our model predicts a difference in -2.17 MPG. That is, our model predicts that manual transmissions are 2.17 MPG more efficient than automatic transmissions. However, this effect did not reach significance, \\(F(1,28)=2.33, p=.138\\). We do not consider this a reliable effect, and so we have to reserve judgment on whether efficiency is related to transmission type. There’s one parameter we have not seen yet: the interaction term, labelled wtC:amC in the output. The parameter shows an estimate of -5.298, and we see that it is significantly different from zero, \\(F(1,28)=13.45, p=.001\\). Recall that this parameter corresponds to \\(b_3\\) in equation (6.2), which represents the amount that weight’s effect depends on transmission type. The fact that it is significantly different from zero tells us that weight’s effect does, in fact, depend on transmission type! 6.3 Interpretting Interactions But how does the relationship change for each tranmission type? There’s a few ways that interactions can play out. We are going to go back to one our old tricks for this: changing where zero falls for predictors. Recall that the “compound” effect of weight in this model is this \\(b_1 + b_3 * amC_i\\). The implication is that when \\(amC_i\\) is zero, the effect of weight is just \\(b_1\\). Where is \\(amC_i\\) zero? When we contrast code it, we know that it is zero for an “average” car. In the interpretation of weight above, we looked at the weight effect just that way, and concluded that the average car shows a relationship between weight and efficiency. But we also have ways of changing what zero means for our predictors. In the code at the beginning of this chapter, I calculated two dummy code. I’m going to re-estimate the model with the dummy code where automatic transmissions are coded zero. int_model_auto = lm( mpg ~ wtC + amAuto + wtC * amAuto, data = mtcars_3 ) lmSummary(int_model_auto) ## ## Call: ## lm(formula = mpg ~ wtC + amAuto + wtC * amAuto, data = mtcars_3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6004 -1.5446 -0.5325 0.9012 6.0909 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 19.2358 0.7357 26.1469 683.6581 0.961 &lt; 2e-16 *** ## wtC -3.7859 0.7856 -4.8188 23.2212 0.453 4.55e-05 *** ## amAuto -2.1677 1.4189 -1.5278 2.3341 0.077 0.13779 ## wtC:amAuto -5.2984 1.4447 -3.6674 13.4502 0.324 0.00102 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.591 on 28 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.8151 ## F-statistic: 46.57 on 3 and 28 DF, p-value: 5.209e-11 There are two parameter interpretations we neeed to change. Notice that the interaction term and the amAuto term have exactly the same magnitudes as in the original model. That’s because changing the zero point of am does not affect those terms, but it does affect the slope for weight and the intercept. Intercept: For cars with automatic transmissions and average weight, the model predicts an efficiency of 19.2 MPG (still significantly different from zero). Weight: for cars with automatic transmissions, the model predicts that car efficiency decreases by 3.78 MPG for every 1 ton weight increase, \\(F(1,28)=23.22, p&lt;.001\\). This relationship is significant for cars with automatic transmissions. We have a way to focus our model on a particular kind of car! Let’s focus it on cars with manual transmissions. int_model_man = lm( mpg ~ wtC + amMan + wtC * amMan, data = mtcars_3 ) lmSummary(int_model_man) ## ## Call: ## lm(formula = mpg ~ wtC + amMan + wtC * amMan, data = mtcars_3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6004 -1.5446 -0.5325 0.9012 6.0909 ## ## Coefficients: ## Estimate Std. Error t value f value R^2 Pr(&gt;|t|) ## (Intercept) 17.068 1.213 14.068 197.907 0.876 3.21e-14 *** ## wtC -9.084 1.212 -7.493 56.142 0.667 3.68e-08 *** ## amMan 2.168 1.419 1.528 2.334 0.077 0.13779 ## wtC:amMan 5.298 1.445 3.667 13.450 0.324 0.00102 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.591 on 28 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.8151 ## F-statistic: 46.57 on 3 and 28 DF, p-value: 5.209e-11 Intercept: cars with manual transmissions and average weight are predicted to have efficiency of 17.07 MPG (signficantly different from zero). Weight: cars with manual transmissions are predicted to decrease in efficiency by 9.08 MPG for every 1 ton increase in weight, \\(F(1,28)=56.14, p&lt;.001\\). The relationship between weight and effiency for cars with manual transmissions is significant. In fact, using the slopes from those two models (but keeping the intercept from the first model), I can put lines on a graph that represent these trends: ## ggplot likes to have dataframes for things, so I contruct one here with the ## slopes and intercepts I want plotTbl &lt;- tribble( ~ Transmission, ~ int, ~ slp, &quot;Automatic&quot;, 19.2358, -3.786, &quot;Manual&quot;, 17.068, -9.084, &quot;On Average&quot;, 18.15, -6.435 ) # Intercepts come from the &quot;on average&quot; model, and then we use the different # slopes estimated for the different groups mtcars_3 %&gt;% ggplot( aes(x = wtC, y = mpg) ) + geom_point() + geom_abline( # automatic transmisions aes(intercept = int, slope = slp, color = Transmission), data = plotTbl ) Now we can see that the plot confirms our interpretations. The red line, representing cars with automatic transmissions, shows a weaker relationship (flatter slope) between weight and efficiency than the green line, representing manual transmissions. Averaging those in the blue line shows a slope somewhere in between. Running the two models with dummy codes is known as exploring simple effects. Here, we explored the simple effect of weight on MPG for each transmission category separately. 6.4 Extension to Continous Moderators The really powerful thing about using regression for this analysis is that we do not need to care so much about whether our predictors are categorical or continuous. In most introductory statistics courses, it is taught that you can have categorical variables interact with an ANOVA. If you have a variable that is continuously valued and one that is categorical, then you need “ANCOVA.” But I’ll claim here that these are all actuall the same model. ANOVA is exactly equivalent to contrast-coding predictors and entering them in a regression, with two main effects and an interaction term. ANCOVA is what we just did in this example. Finally, it is not a problem to allow two variables, both continuously valued, to interact! We do need to think about where zero is in the same way as before. So, mean center your predictors, and enter them in the model just like we did above. To explore the simple effects, we conceptually do the same thing, but we can’t construct dummy codes for continuous variables. Instead, it is common practice to center one of the interacting variables at 1 standard deviation above the mean, and then again at 1 standard deviation below the mean. Then, estimate the simple effects models like I estimated the dummy-coded ones here. Interpret accordingly! "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
